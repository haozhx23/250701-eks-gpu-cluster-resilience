apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: llama3-1b-fsdp-t1
spec:
  elasticPolicy:
    rdzvBackend: c10d
    minReplicas: 1
    maxReplicas: 64
    maxRestarts: 100
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 90
  pytorchReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: llama3-1b-fsdp
        spec:
          nodeSelector:
            node.kubernetes.io/instance-type: "g6.12xlarge"
          volumes:
            - name: shmem
              hostPath: 
                path: /dev/shm
            - name: local
              hostPath:
                path: /mnt/k8s-disks/0
            - name: persistent-storage
              persistentVolumeClaim:
                claimName: fsx-claim
          containers:
            - name: pytorch
              image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.7.1-gpu-py312-cu128-ubuntu22.04-ec2
              imagePullPolicy: Always
              resources:
                requests:
                  nvidia.com/gpu: 4
                  vpc.amazonaws.com/efa: 1
                limits:
                  nvidia.com/gpu: 4
                  vpc.amazonaws.com/efa: 1
              env:
              # for P5 FI_* should be commented out
              - name: LOGLEVEL
                value: "DEBUG"
              - name: FI_PROVIDER
                value: "efa"
              #- name: FI_EFA_USE_DEVICE_RDMA
              #  value: "1"
              #- name: FI_EFA_FORK_SAFE
              #  value: "1"
              #- name: FI_LOG_LEVEL
              #  value: "1"
              #- name: FI_EFA_ENABLE_SHM_TRANSFER
              #  value: "1"
              - name: TORCH_DISTRIBUTED_DEBUG
                value: "DETAIL"
              - name: TORCH_NCCL_ENABLE_MONITORING
                value: "1"
              - name: TORCH_NCCL_TRACE_BUFFER_SIZE
                value: "20000"
              - name: TORCH_NCCL_DUMP_ON_TIMEOUT
                value: "1"
              - name: TORCH_NCCL_DEBUG_INFO_TEMP_FILE
                value: "/local/nccl_trace_rank_"
              - name: PYTORCH_CUDA_ALLOC_CONF
                value: "expandable_segments:True"
              - name: NCCL_DEBUG
                value: "INFO"
              - name: NCCL_SOCKET_IFNAME
                value: "^lo"
              - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
                value: "1"
              - name: HF_TOKEN
                value: "<YOUR_HUGGING_FACE_TOKEN_HERE>"
              #- name: TORCH_DIST_INIT_BARRIER
              #  value: "1"
              #- name: NCCL_IGNORE_DISABLED_P2P
              #  value: "1"
              #- name: NCCL_NVLS_ENABLE
              #  value: "0"
              command: ["bash", "/dfsx/FSDP-k8s/fsdp-entry.sh"]
              volumeMounts:
                - name: shmem
                  mountPath: /dev/shm
                - name: local
                  mountPath: /local
                - name: persistent-storage
                  mountPath: /dfsx
