#!/usr/bin/env python3
"""
DCGM GPUç›‘æ§è„šæœ¬ - Pythonç‰ˆæœ¬
"""

import os
import sys
import time
import signal
import subprocess
import logging
from datetime import datetime
from typing import Optional, Tuple, Dict, Any

# æ·»åŠ libç›®å½•åˆ°Pythonè·¯å¾„
script_dir = os.path.dirname(os.path.abspath(__file__))
lib_dir = os.path.join(script_dir, 'lib')
sys.path.insert(0, lib_dir)

from exclusion_manager import ExclusionManager
from lib.handlers.error_dispatch import ErrorHandlerDispatch

class DCGMMonitor:
    def __init__(self):
        """åˆå§‹åŒ–DCGMç›‘æ§å™¨"""
        self.script_dir = script_dir
        self.lib_dir = lib_dir
        
        # é…ç½®å‚æ•°
        self.monitor_interval = int(os.getenv('MONITOR_INTERVAL', '30'))
        self.log_level = os.getenv('LOG_LEVEL', 'DEBUG')
        self.webhook_url = os.getenv('WEBHOOK_URL', '')
        self.lambda_function = os.getenv('LAMBDA_FUNCTION', '')
        
        # åˆå§‹åŒ–æ’é™¤ç®¡ç†å™¨
        self.exclusion_manager = ExclusionManager()

                
        self.error_handlers = ErrorHandlerDispatch()

        
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logger()
        
        # ä¿¡å·å¤„ç†
        signal.signal(signal.SIGTERM, self._signal_handler)
        signal.signal(signal.SIGINT, self._signal_handler)
        
        self.running = True
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(f'DCGMMonitor.{self.__class__.__name__}')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
            # è®¾ç½®æ—¥å¿—çº§åˆ«
            level = getattr(logging, self.log_level.upper(), logging.INFO)
            logger.setLevel(level)
        
        return logger
    
    def _signal_handler(self, signum, frame):
        self.logger.info("STOP Signal received and Stopping service...")
        self.running = False
    
    def _run_shell_command(self, command: str, shell_file: str = None) -> Tuple[bool, str]:
        """
        æ‰§è¡Œshellå‘½ä»¤æˆ–è°ƒç”¨shellå‡½æ•°
        
        Args:
            command: è¦æ‰§è¡Œçš„å‘½ä»¤
            shell_file: å¦‚æœéœ€è¦sourceç‰¹å®šæ–‡ä»¶ï¼ŒæŒ‡å®šæ–‡ä»¶è·¯å¾„
        
        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, è¾“å‡ºå†…å®¹)
        """
        try:
            if shell_file:
                # éœ€è¦sourceæ–‡ä»¶åæ‰§è¡Œå‘½ä»¤
                full_command = f"bash -c 'source {shell_file} && {command}'"
            else:
                full_command = command
            
            self.logger.debug(f"æ‰§è¡Œå‘½ä»¤: {full_command}")
            
            result = subprocess.run(
                full_command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=60  # 60ç§’è¶…æ—¶
            )
            
            if result.returncode != 0 and result.stderr:
                self.logger.debug(f"å‘½ä»¤stderr: {result.stderr}")
            
            return result.returncode == 0, result.stdout.strip()
            
        except subprocess.TimeoutExpired:
            self.logger.error(f"å‘½ä»¤æ‰§è¡Œè¶…æ—¶: {command}")
            return False, ""
        except Exception as e:
            self.logger.error(f"æ‰§è¡Œå‘½ä»¤å¤±è´¥: {command}, é”™è¯¯: {e}")
            return False, ""
    
    def get_dcgm_pods_with_nodes(self) -> list:
        """è·å–DCGM Podå’ŒèŠ‚ç‚¹ä¿¡æ¯"""
        gpu_utils_file = os.path.join(self.lib_dir, 'gpu-utils.sh')
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(gpu_utils_file):
            self.logger.error(f"GPUå·¥å…·æ–‡ä»¶ä¸å­˜åœ¨: {gpu_utils_file}")
            return []
        
        self.logger.debug(f"è°ƒç”¨shellå‡½æ•°: get_dcgm_pods_with_nodes from {gpu_utils_file}")
        success, output = self._run_shell_command('get_dcgm_pods_with_nodes', gpu_utils_file)
        
        self.logger.debug(f"Shellå‡½æ•°è°ƒç”¨ç»“æœ: success={success}, output_length={len(output) if output else 0}")
        
        if not success:
            self.logger.error("æ— æ³•è·å–DCGM Podä¿¡æ¯")
            return []
        
        if not output:
            self.logger.warning("Shellå‡½æ•°è¿”å›ç©ºè¾“å‡º")
            return []
        
        pods_info = []
        for line in output.split('\n'):
            line = line.strip()
            if '|' in line:
                parts = line.split('|')
                if len(parts) >= 2:
                    pod_name = parts[0].strip()
                    node_name = parts[1].strip()
                    if pod_name and node_name:
                        pods_info.append((pod_name, node_name))
                        self.logger.debug(f"æ‰¾åˆ°Pod: {pod_name} on {node_name}")
        
        self.logger.info(f"æ‰¾åˆ° {len(pods_info)} ä¸ªDCGM Pod")
        return pods_info
    
    def get_instance_id(self, node_name: str) -> str:
        """è·å–èŠ‚ç‚¹çš„å®ä¾‹ID"""
        gpu_utils_file = os.path.join(self.lib_dir, 'gpu-utils.sh')
        success, output = self._run_shell_command(f'get_instance_id "{node_name}"', gpu_utils_file)
        
        if success and output:
            return output.strip()
        else:
            self.logger.warning(f"æ— æ³•è·å–èŠ‚ç‚¹ {node_name} çš„å®ä¾‹ID")
            return node_name  # fallback to node name
    
    def query_dcgm_metrics(self, pod_name: str, node_name: str) -> Optional[str]:
        """æŸ¥è¯¢DCGMæŒ‡æ ‡"""
        gpu_utils_file = os.path.join(self.lib_dir, 'gpu-utils.sh')
        command = f'query_dcgm_metrics_multi_gpu "{pod_name}" "{node_name}"'
        
        self.logger.debug(f"æŸ¥è¯¢DCGMæŒ‡æ ‡: {command}")
        success, output = self._run_shell_command(command, gpu_utils_file)
        
        if success:
            self.logger.debug(f"æˆåŠŸè·å–æŒ‡æ ‡ï¼Œè¾“å‡ºé•¿åº¦: {len(output)}")
            return output
        else:
            self.logger.error(f"æ— æ³•æŸ¥è¯¢Pod {pod_name} çš„æŒ‡æ ‡")
            # æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            self.logger.debug(f"æŸ¥è¯¢å¤±è´¥çš„è¾“å‡º: {output}")
            return None
    
    def process_metrics(self, metrics: str, node_name: str, instance_id: str) -> int:
        """
        å¤„ç†æŒ‡æ ‡æ•°æ®
        
        Returns:
            int: é”™è¯¯çº§åˆ« (0=æ­£å¸¸, 1=è­¦å‘Š, 2=é”™è¯¯)
        """
        metrics_processor_file = os.path.join(self.lib_dir, 'metrics-processor-multi-gpu.sh')
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['LOG_LEVEL'] = self.log_level
        env['WEBHOOK_URL'] = self.webhook_url
        env['LAMBDA_FUNCTION'] = self.lambda_function
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨metricsæ•°æ®
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:
                f.write(metrics)
                temp_file = f.name
            
            # è°ƒç”¨metricså¤„ç†å‡½æ•°
            command = f'bash -c \'source "{metrics_processor_file}" && process_metrics "$(cat {temp_file})" "{node_name}" "{instance_id}"\''
            
            self.logger.debug(f"å¤„ç†æŒ‡æ ‡å‘½ä»¤: {command}")
            
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                env=env,
                timeout=120
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file)
            
            # è¾“å‡ºå¤„ç†ç»“æœ
            if result.stdout:
                print(result.stdout)
            if result.stderr:
                self.logger.warning(f"å¤„ç†æŒ‡æ ‡æ—¶çš„è­¦å‘Š: {result.stderr}")
            
            # æ ¹æ®è¾“å‡ºåˆ¤æ–­é”™è¯¯çº§åˆ«
            output = result.stdout.lower()
            if "ä¸¥é‡é—®é¢˜" in output or "ç´§æ€¥å¤„ç†" in output:
                return 2  # é”™è¯¯
            elif "è­¦å‘Š" in output or "ç›‘æ§å‘Šè­¦" in output:
                return 1  # è­¦å‘Š
            else:
                return 0  # æ­£å¸¸
                
        except Exception as e:
            self.logger.error(f"å¤„ç†æŒ‡æ ‡æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return 0
    

    def process_metrics_llm(self, metrics: str) -> dict:
        from lib.metrics_processor_llm import parse_gpu_metric_info
        parse_result = parse_gpu_metric_info(metrics)

        if isinstance(parse_result, dict):
            # parse_result['node_name'] = node_name
            # parse_result['instance_id'] = instance_id
            self.logger.debug(f"Parsed dict metric by LLM: {parse_result}")
            return parse_result
        else:
            raise ValueError(f"LLM Result Parse Failed: {parse_result}")

        

    def handle_gpu_error(self, error_level: int, node_name: str, instance_id: str, error_type: str = "UNKNOWN"):
        """
        å¤„ç†GPUé”™è¯¯ï¼Œæ·»åŠ åˆ°æ’é™¤åˆ—è¡¨
        
        Args:
            error_level: é”™è¯¯çº§åˆ«
            node_name: èŠ‚ç‚¹åç§°
            instance_id: å®ä¾‹ID
            error_type: é”™è¯¯ç±»å‹
        """
        if error_level >= 2:  # ä¸¥é‡é”™è¯¯
            # æ·»åŠ åˆ°æ’é™¤åˆ—è¡¨ï¼Œæ’é™¤æ—¶é—´æ›´é•¿
            timeout = 3600  # 1å°æ—¶
            self.exclusion_manager.add_exclusion(instance_id, node_name, f"CRITICAL_{error_type}", timeout)
            self.logger.critical(f"ğŸš¨ ä¸¥é‡GPUé”™è¯¯ï¼Œå®ä¾‹ {instance_id} å·²æš‚åœç›‘æ§1å°æ—¶")
            
        elif error_level >= 1:  # è­¦å‘Š
            # æ·»åŠ åˆ°æ’é™¤åˆ—è¡¨ï¼Œæ’é™¤æ—¶é—´è¾ƒçŸ­
            timeout = 1800  # 30åˆ†é’Ÿ
            self.exclusion_manager.add_exclusion(instance_id, node_name, f"WARNING_{error_type}", timeout)
            self.logger.warning(f"âš ï¸  GPUè­¦å‘Šï¼Œå®ä¾‹ {instance_id} å·²æš‚åœç›‘æ§30åˆ†é’Ÿ")

    # def reboot_gpu_instance_async(self, node_name, instance_id, wait_time=1800):
    #     script_path = "./lib/handlers/gpu-instance-reboot-test.sh"
    #     cmd = [script_path, "run", node_name, instance_id, str(wait_time)]
        
    #     try:
    #         process = subprocess.Popen(
    #             cmd,
    #             stdout=subprocess.PIPE,
    #             stderr=subprocess.PIPE,
    #             text=True
    #         )
            
    #         print(f"é‡å¯å·²è§¦å‘ï¼ŒPID: {process.pid}, Node Name: {node_name}")
    #         return process.pid
            
    #     except Exception as e:
    #         print(f"å¯åŠ¨å¤±è´¥: {e}")
    #         return None

    def handle_gpu_error(self, parsed_error: dict, node_name: str, instance_id: str):
        # {'error_class': 'XID_ERROR', 'error_count': 1, 'error_gpu_id': 0}
        error_class = parsed_error['error_class']
        error_count = parsed_error['error_count']
        
        if "HEALTHY" != error_class:
            result = self.error_handlers.send_cloudwatch_metrics(error_class, error_count, node_name, instance_id)

            if result.get('status') == 'success':
                self.logger.debug(f"Customized CloudWatch Metric Sent.")
            else:
                self.logger.debug(f"Customized CloudWatch Metric Sent Failed: {result.get('message', 'Unknown error')}")
            

        if "XID" in error_class and error_count > 5:
            # timeout = 1800
            timeout = 900
            self.exclusion_manager.add_exclusion(instance_id, node_name, f"{error_class}", timeout)
            self.logger.critical(f"CRITICAL GPU ERROR {instance_id} - {parsed_error}; Pause monitoring {timeout}s for processing.")

            result = self.error_handlers.call_replace_script(node_name, instance_id, timeout)

        elif "XID" in error_class and error_count <= 5:
            
            timeout = 200
            self.exclusion_manager.add_exclusion(instance_id, node_name, f"{error_class}", timeout)
            self.logger.error(f"OTHER GPU ERROR {instance_id} - {parsed_error}; Pause monitoring {timeout}s for processing.")

            result = self.error_handlers.call_reboot_script(node_name, instance_id, 30, True)

        else:
            self.logger.debug(f"{error_class} detected & keep running")
            

    ## 
    def monitor_single_node(self, pod_name: str, node_name: str) -> bool:
        """
        ç›‘æ§å•ä¸ªèŠ‚ç‚¹
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸç›‘æ§
        """
        instance_id = self.get_instance_id(node_name)
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç›‘æ§æ­¤å®ä¾‹
        should_monitor, skip_reason = self.exclusion_manager.should_monitor(instance_id, node_name)
        
        if not should_monitor:
            # å®ä¾‹è¢«æ’é™¤ï¼Œè·³è¿‡ç›‘æ§
            return True
        
        self.logger.info(f"ğŸ“¡ ç›‘æ§èŠ‚ç‚¹: {node_name} ({instance_id}) - Pod: {pod_name}")
        
        # æŸ¥è¯¢æŒ‡æ ‡
        metrics = self.query_dcgm_metrics(pod_name, node_name)

        self.logger.debug(f"query_dcgm_metrics description text: \n{metrics}")

        if not metrics:
            self.logger.error(f"Failed get metrics from {pod_name}.")
            return False
        
        # # å¤„ç†æŒ‡æ ‡
        # error_level = self.process_metrics(metrics, node_name, instance_id)
        
        # # å¦‚æœæœ‰é”™è¯¯ï¼Œæ·»åŠ åˆ°æ’é™¤åˆ—è¡¨
        # if error_level > 0:
        #     self.handle_gpu_error(error_level, node_name, instance_id)

        error_detail = self.process_metrics_llm(metrics)
        if "HEALTHY" != error_detail['error_class']:
            self.handle_gpu_error(error_detail, node_name, instance_id)
        
        return True
    
    def monitor_all_nodes(self):
        """ç›‘æ§æ‰€æœ‰GPUèŠ‚ç‚¹"""
        self.logger.info("Scan all GPU Nodes ...")
        
        # è·å–æ‰€æœ‰DCGM Podä¿¡æ¯
        pods_info = self.get_dcgm_pods_with_nodes()
        
        if not pods_info:
            self.logger.warning("DCGM Pods NOT Found")
            return
        
        # ç›‘æ§æ¯ä¸ªèŠ‚ç‚¹
        for pod_name, node_name in pods_info:
            try:
                self.monitor_single_node(pod_name, node_name)
                print("---")
            except Exception as e:
                self.logger.error(f"Exception when observing node - {node_name} : {e}")
    
    def show_startup_info(self):
        print("ğŸš€ DCGM GPU Oberving Start ...")
        print(f"Observation Time Interval: {self.monitor_interval} secs")
        print(f"Observation Logging Level: {self.log_level}")
        
        if self.lambda_function:
            print(f"Lambda Function: {self.lambda_function}")
        if self.webhook_url:
            print(f"Webhook URL: {self.webhook_url}")
        
        print("")
    
    def run(self):
        """ä¸»ç›‘æ§å¾ªç¯"""
        self.show_startup_info()
        
        while self.running:
            time.sleep(5)

            try:
                self.monitor_all_nodes()
                
                if self.running:  # æ£€æŸ¥æ˜¯å¦ä»åœ¨è¿è¡Œ
                    self.logger.info(f"Wait {self.monitor_interval} secs Interval ...")
                    time.sleep(self.monitor_interval)
                    
            except KeyboardInterrupt:
                self.logger.info("Received keyboard interrupt signal..")
                break
            except Exception as e:
                self.logger.error(f"exception occurred in monitoring loop: {e}")
                if self.running:
                    time.sleep(10)
        
        self.logger.info("Main Monitoring Stopped")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='DCGM GPUç›‘æ§å™¨ - Pythonç‰ˆæœ¬')
    parser.add_argument('--interval', type=int, help='ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--webhook-url', help='Webhook URL')
    parser.add_argument('--lambda-function', help='Lambdaå‡½æ•°åç§°')
    
    # æ’é™¤ç®¡ç†å‘½ä»¤
    parser.add_argument('--exclusion-cmd', choices=['list', 'cleanup', 'pause', 'resume'], 
                       help='æ’é™¤ç®¡ç†å‘½ä»¤')
    parser.add_argument('--instance-id', help='å®ä¾‹IDï¼ˆç”¨äºæ’é™¤ç®¡ç†ï¼‰')
    parser.add_argument('--node-name', help='èŠ‚ç‚¹åç§°ï¼ˆç”¨äºæ’é™¤ç®¡ç†ï¼‰')
    parser.add_argument('--reason', help='æš‚åœåŸå› ï¼ˆç”¨äºæ’é™¤ç®¡ç†ï¼‰')
    
    args = parser.parse_args()
    
    # å¤„ç†æ’é™¤ç®¡ç†å‘½ä»¤
    if args.exclusion_cmd:
        manager = ExclusionManager()
        
        if args.exclusion_cmd == 'list':
            manager.show_status()
        elif args.exclusion_cmd == 'cleanup':
            count = manager.cleanup_expired()
            print(f"æ¸…ç†äº† {count} ä¸ªè¿‡æœŸè®°å½•")
        elif args.exclusion_cmd == 'pause':
            if not args.instance_id or not args.node_name:
                print("æš‚åœç›‘æ§éœ€è¦ --instance-id å’Œ --node-name å‚æ•°")
                return 1
            reason = args.reason or "MANUAL_PAUSE"
            manager.pause_instance(args.instance_id, args.node_name, reason)
        elif args.exclusion_cmd == 'resume':
            if not args.instance_id:
                print("æ¢å¤ç›‘æ§éœ€è¦ --instance-id å‚æ•°")
                return 1
            manager.resume_instance(args.instance_id)
        
        return 0
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æä¾›ï¼‰
    if args.interval:
        os.environ['MONITOR_INTERVAL'] = str(args.interval)
    if args.log_level:
        os.environ['LOG_LEVEL'] = args.log_level
    if args.webhook_url:
        os.environ['WEBHOOK_URL'] = args.webhook_url
    if args.lambda_function:
        os.environ['LAMBDA_FUNCTION'] = args.lambda_function
    
    # å¯åŠ¨ç›‘æ§
    monitor = DCGMMonitor()
    try:
        monitor.run()
        return 0
    except Exception as e:
        print(f"Observation Launch Failed: {e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())
